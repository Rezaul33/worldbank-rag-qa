{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG System Evaluation Notebook\n",
    "\n",
    "This notebook evaluates the World Bank RAG system performance with various metrics and logging.\n",
    "\n",
    "## Evaluation Components:\n",
    "1. **Retrieval Quality** - Relevance and similarity scores\n",
    "2. **Answer Quality** - Coherence, accuracy, citation quality\n",
    "3. **Performance Metrics** - Response time, success rates\n",
    "4. **Error Analysis** - Failure patterns and edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Import our RAG components\n",
    "from retriever.rag_retriever import RAGRetriever\n",
    "from generator.answer_generator import AnswerGenerator\n",
    "from vector_store.chroma_db import ChromaVectorStore\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Evaluation environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Questions Setup\n",
    "\n",
    "Define evaluation questions covering different aspects of World Bank reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation questions with expected topics\n",
    "evaluation_questions = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"question\": \"What are the main challenges in global development?\",\n",
    "        \"expected_topics\": [\"development challenges\", \"global issues\", \"policy\"],\n",
    "        \"difficulty\": \"easy\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"question\": \"How does climate change affect developing countries?\",\n",
    "        \"expected_topics\": [\"climate change\", \"developing countries\", \"environmental impact\"],\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"question\": \"What are the recommendations for economic growth?\",\n",
    "        \"expected_topics\": [\"economic growth\", \"policy recommendations\", \"development strategies\"],\n",
    "        \"difficulty\": \"medium\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"question\": \"How do global value chains impact development?\",\n",
    "        \"expected_topics\": [\"global value chains\", \"GVCs\", \"trade\", \"development\"],\n",
    "        \"difficulty\": \"hard\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"question\": \"What are the effects of technological change on developing economies?\",\n",
    "        \"expected_topics\": [\"technological change\", \"innovation\", \"economic impact\"],\n",
    "        \"difficulty\": \"hard\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(evaluation_questions)} evaluation questions\")\n",
    "for q in evaluation_questions:\n",
    "    print(f\"{q['id']}. {q['question']} ({q['difficulty']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG retriever\n",
    "print(\"Initializing RAG system...\")\n",
    "retriever = RAGRetriever(ollama_model=\"llama2:latest\")\n",
    "\n",
    "# Initialize answer generator for quality assessment\n",
    "answer_generator = AnswerGenerator()\n",
    "\n",
    "# Check Ollama connection\n",
    "if not retriever.check_ollama_connection():\n",
    "    print(\"ERROR: Cannot connect to Ollama. Please ensure Ollama is running.\")\n",
    "    print(\"Run: ollama serve\")\n",
    "else:\n",
    "    print(\"RAG system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Evaluation\n",
    "\n",
    "Execute all evaluation questions and collect metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "evaluation_results = []\n",
    "\n",
    "for question_data in evaluation_questions:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating Question {question_data['id']}: {question_data['question']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Run RAG query\n",
    "    start_time = time.time()\n",
    "    result = retriever.answer_query(question_data['question'], top_k=5)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Store result with metadata\n",
    "    evaluation_entry = {\n",
    "        \"question_id\": question_data['id'],\n",
    "        \"question\": question_data['question'],\n",
    "        \"expected_topics\": question_data['expected_topics'],\n",
    "        \"difficulty\": question_data['difficulty'],\n",
    "        \"answer\": result.get('answer', ''),\n",
    "        \"sources\": result.get('sources', []),\n",
    "        \"retrieval_time\": result.get('retrieval_time', 0),\n",
    "        \"documents_retrieved\": result.get('documents_retrieved', 0),\n",
    "        \"error\": result.get('error', None),\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Assess answer quality\n",
    "    if result.get('answer'):\n",
    "        quality_assessment = answer_generator.assess_answer_quality(\n",
    "            result['answer'], \n",
    "            question_data['question'], \n",
    "            result.get('sources', [])\n",
    "        )\n",
    "        evaluation_entry.update(quality_assessment)\n",
    "    \n",
    "    evaluation_results.append(evaluation_entry)\n",
    "    \n",
    "    print(f\"Retrieval time: {evaluation_entry['retrieval_time']:.2f}s\")\n",
    "    print(f\"Documents retrieved: {evaluation_entry['documents_retrieved']}\")\n",
    "    print(f\"Overall quality score: {evaluation_entry.get('overall_score', 0):.3f}\")\n",
    "    \n",
    "    if evaluation_entry.get('error'):\n",
    "        print(f\"Error: {evaluation_entry['error']}\")\n",
    "    \n",
    "    # Brief answer preview\n",
    "    answer_preview = evaluation_entry['answer'][:200] + \"...\" if len(evaluation_entry['answer']) > 200 else evaluation_entry['answer']\n",
    "    print(f\"Answer preview: {answer_preview}\")\n",
    "\n",
    "print(f\"\\nEvaluation complete! Processed {len(evaluation_results)} questions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(evaluation_results)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"=== EVALUATION SUMMARY ====\")\n",
    "print(f\"Total questions evaluated: {len(df)}\")\n",
    "print(f\"Successful responses: {len(df[df['error'].isna()])}\")\n",
    "print(f\"Failed responses: {len(df[df['error'].notna()])}\")\n",
    "print(f\"Success rate: {len(df[df['error'].isna()])/len(df)*100:.1f}%\")\n",
    "\n",
    "# Performance metrics\n",
    "avg_retrieval_time = df['retrieval_time'].mean()\n",
    "avg_quality_score = df['overall_score'].mean()\n",
    "avg_docs_retrieved = df['documents_retrieved'].mean()\n",
    "\n",
    "print(f\"\\n=== PERFORMANCE METRICS ====\")\n",
    "print(f\"Average retrieval time: {avg_retrieval_time:.2f} seconds\")\n",
    "print(f\"Average quality score: {avg_quality_score:.3f}\")\n",
    "print(f\"Average documents retrieved: {avg_docs_retrieved:.1f}\")\n",
    "\n",
    "# Display detailed results\n",
    "print(\"\\n=== DETAILED RESULTS ====\")\n",
    "display_cols = ['question_id', 'difficulty', 'retrieval_time', 'documents_retrieved', 'overall_score', 'length_score', 'relevance_score', 'citation_score']\n",
    "print(df[display_cols].round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('RAG System Evaluation Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Retrieval Time Distribution\n",
    "axes[0, 0].hist(df['retrieval_time'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Retrieval Time Distribution')\n",
    "axes[0, 0].set_xlabel('Time (seconds)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Quality Score Distribution\n",
    "axes[0, 1].hist(df['overall_score'], bins=10, alpha=0.7, color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Answer Quality Score Distribution')\n",
    "axes[0, 1].set_xlabel('Quality Score')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Performance by Difficulty\n",
    "difficulty_perf = df.groupby('difficulty')[['retrieval_time', 'overall_score']].mean()\n",
    "x = range(len(difficulty_perf))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar([i - width/2 for i in x], difficulty_perf['retrieval_time'], width, label='Retrieval Time', alpha=0.7, color='orange')\n",
    "axes[1, 0].bar([i + width/2 for i in x], difficulty_perf['overall_score'], width, label='Quality Score', alpha=0.7, color='purple')\n",
    "axes[1, 0].set_title('Performance by Question Difficulty')\n",
    "axes[1, 0].set_xlabel('Difficulty Level')\n",
    "axes[1, 0].set_ylabel('Average Value')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels(difficulty_perf.index)\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Documents Retrieved vs Quality Score\n",
    "axes[1, 1].scatter(df['documents_retrieved'], df['overall_score'], alpha=0.7, color='red', s=60)\n",
    "axes[1, 1].set_title('Documents Retrieved vs Quality Score')\n",
    "axes[1, 1].set_xlabel('Documents Retrieved')\n",
    "axes[1, 1].set_ylabel('Quality Score')\n",
    "\n",
    "# Add correlation line\n",
    "z = np.polyfit(df['documents_retrieved'], df['overall_score'], 1)\n",
    "p = np.poly1d(z)\n",
    "axes[1, 1].plot(df['documents_retrieved'], p(df['documents_retrieved']), \"r--\", alpha=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze errors and issues\n",
    "error_df = df[df['error'].notna()]\n",
    "low_quality_df = df[df['overall_score'] < 0.5]\n",
    "slow_responses_df = df[df['retrieval_time'] > 30]\n",
    "\n",
    "print(\"=== ERROR ANALYSIS ====\")\n",
    "print(f\"Questions with errors: {len(error_df)}\")\n",
    "print(f\"Questions with low quality (< 0.5): {len(low_quality_df)}\")\n",
    "print(f\"Slow responses (> 30s): {len(slow_responses_df)}\")\n",
    "\n",
    "# Show error details\n",
    "if len(error_df) > 0:\n",
    "    print(\"\\nError Details:\")\n",
    "    for _, row in error_df.iterrows():\n",
    "        print(f\"Q{row['question_id']}: {row['error']}\")\n",
    "\n",
    "# Show low quality answers\n",
    "if len(low_quality_df) > 0:\n",
    "    print(\"\\nLow Quality Answers:\")\n",
    "    for _, row in low_quality_df.iterrows():\n",
    "        print(f\"Q{row['question_id']} (Score: {row['overall_score']:.3f}): {row['question'][:50]}...\")\n",
    "\n",
    "# Common issues\n",
    "all_issues = []\n",
    "for _, row in df.iterrows():\n",
    "    if row.get('issues'):\n",
    "        all_issues.extend(row['issues'])\n",
    "\n",
    "if all_issues:\n",
    "    from collections import Counter\n",
    "    issue_counts = Counter(all_issues)\n",
    "    print(\"\\nCommon Issues:\")\n",
    "    for issue, count in issue_counts.most_common():\n",
    "        print(f\"  {issue}: {count} occurrences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "results_file = f'evaluation_results_{timestamp}.json'\n",
    "csv_file = f'evaluation_results_{timestamp}.csv'\n",
    "\n",
    "# Save as JSON\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "print(f\"Results saved to: {results_file}\")\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"CSV saved to: {csv_file}\")\n",
    "\n",
    "# Generate summary report\n",
    "summary_report = f\"\"\"\n",
    "# RAG System Evaluation Report\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Summary\n",
    "- Total Questions: {len(df)}\n",
    "- Success Rate: {len(df[df['error'].isna()])/len(df)*100:.1f}%\n",
    "- Average Retrieval Time: {avg_retrieval_time:.2f}s\n",
    "- Average Quality Score: {avg_quality_score:.3f}\n",
    "- Average Documents Retrieved: {avg_docs_retrieved:.1f}\n",
    "\n",
    "## Performance by Difficulty\n",
    "{difficulty_perf.to_string()}\n",
    "\n",
    "## Issues Identified\n",
    "- Errors: {len(error_df)}\n",
    "- Low Quality Answers: {len(low_quality_df)}\n",
    "- Slow Responses: {len(slow_responses_df)}\n",
    "\n",
    "## Recommendations\n",
    "1. {'Improve Ollama response time' if len(slow_responses_df) > 0 else 'Response times are acceptable'}\n",
    "2. {'Review answer quality for difficult questions' if len(low_quality_df) > 0 else 'Answer quality is good'}\n",
    "3. {'Investigate error patterns' if len(error_df) > 0 else 'System is stable'}\n",
    "\"\"\"\n",
    "\n",
    "summary_file = f'evaluation_summary_{timestamp}.txt'\n",
    "with open(summary_file, 'w') as f:\n",
    "    f.write(summary_report)\n",
    "\n",
    "print(f\"\\nSummary report saved to: {summary_file}\")\n",
    "print(\"\\nEvaluation complete! Check the generated files for detailed analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
