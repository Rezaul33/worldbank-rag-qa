# ğŸ¦ World Bank RAG QA System

A sophisticated Retrieval-Augmented Generation (RAG) system that provides intelligent question-answering capabilities for World Bank Development Reports using advanced vector search and AI-powered response generation.

## ğŸ¯ Overview

This system transforms static World Bank Development Reports into an interactive knowledge base, allowing users to ask natural language questions and receive accurate, context-aware answers with proper citations. It combines modern embedding techniques, vector databases, and large language models to create a powerful research assistant.

## ğŸ—ï¸ Architecture

```mermaid
flowchart LR
    A["PDF Reports 2020-2025"] --> B["Text Chunking & Processing"]
    B --> C["Embeddings Generation"]
    C --> D["Vector Store - ChromaDB"]
    D --> E["RAG Pipeline & Answer Generation"]
    E --> F["LLM - Ollama (Llama2/3)"]
    E --> G["Web Interface - Streamlit"]
```

## ğŸ“Š Dataset

### Source Documents
- **World Development Report 2020**: Trading for Development in the Age of Global Value Chains
- **World Development Report 2021**: Data, Digitalization, and Development
- **World Development Report 2022**: Finance for an Equitable Recovery
- **World Development Report 2023**: Migrants, Refugees, and Societies
- **World Development Report 2024**: The Middle Income Trap
- **World Development Report 2025**: Reimagining Development

### Data Processing Pipeline

1. **PDF Ingestion**: Using `pdfplumber` for accurate text extraction
2. **Text Chunking**: 
   - Intelligent chunking based on semantic boundaries
   - Chunk size: ~1000 characters with overlap
   - Preserves context and maintains readability
3. **Metadata Extraction**:
   - Document filename and page numbers
   - Character and token counts
   - Chunk indexing for precise citation

### Embedding Generation
- **Model**: `all-MiniLM-L6-v2` (SentenceTransformers)
- **Dimensions**: 384-dimensional vectors
- **Technique**: Semantic embeddings capturing contextual meaning
- **Total Chunks**: 3,010 document fragments

## ğŸ—„ï¸ Database Technology

### ChromaDB Vector Store
- **Type**: Persistent vector database
- **Storage**: Local file system (`vector_store/chroma_db/`)
- **Similarity Search**: Cosine similarity for semantic matching
- **Metadata Support**: Rich metadata for filtering and citation
- **Scalability**: Handles thousands of document chunks efficiently

### Search Strategy
- **Query Processing**: Natural language â†’ embedding vector
- **Retrieval**: Top-k most similar chunks (default k=5)
- **Similarity Scoring**: Normalized cosine similarity (0-1)
- **Context Assembly**: Retrieved chunks formatted for LLM consumption

## ğŸ¤– AI Integration

### Large Language Model
- **Backend**: Ollama (local LLM serving)
- **Models**: Llama2, Llama3, and other compatible models
- **Settings**: Temperature=0.1, max_tokens=1000
- **Prompt Engineering**: Structured prompts for consistent, accurate responses

### Answer Generation Pipeline
1. **Query Understanding**: Parse user intent
2. **Document Retrieval**: Find relevant chunks
3. **Context Assembly**: Format retrieved information
4. **LLM Generation**: Generate contextualized response
5. **Quality Assessment**: Evaluate answer relevance and completeness

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.8 or higher
- Ollama installed and running
- Git (for cloning)

### Step 1: Clone Repository
```bash
git clone https://github.com/Rezaul33/worldbank-rag-qa.git
cd worldbank-rag-qa
```

### Step 2: Install Python Dependencies
```bash
pip install -r requirements.txt
```

### Step 3: Install and Start Ollama
```bash
# Install Ollama (if not already installed)
# Visit: https://ollama.ai/

# Start Ollama server
ollama serve

# Pull a model (in another terminal)
ollama pull llama2
# or
ollama pull llama3
```

### Step 4: Verify Installation
```bash
# Test Ollama connection
python test_ollama_connection.py

# Test RAG pipeline
python test_rag_simple.py
```

## ğŸ–¥ï¸ Usage

### Starting the Web Interface
```bash
# Option 1: Using the runner script (recommended)
python app/run_streamlit.py

# Option 2: Direct Streamlit launch
streamlit run app/streamlit_app.py
```

### Using the System
1. **Open Browser**: Navigate to `http://localhost:8501`
2. **Select Model**: Choose your preferred Ollama model in the sidebar
3. **Ask Questions**: Type questions in natural language
4. **View Results**: Get answers with source citations and similarity scores

### Example Queries
- "What are the main challenges in global development?"
- "How does climate change affect developing countries?"
- "What are the recommendations for economic growth?"
- "How do global value chains impact development?"

## ğŸ“ Project Structure

```
worldbank-rag-qa/
â”œâ”€â”€ app/                          # Streamlit web interface
â”‚   â”œâ”€â”€ streamlit_app.py          # Main web application
â”‚   â””â”€â”€ run_streamlit.py          # Runner with error handling
â”œâ”€â”€ chunking/                     # Text processing modules
â”‚   â””â”€â”€ text_chunker.py           # Document chunking logic
â”œâ”€â”€ embeddings/                   # Embedding generation
â”‚   â””â”€â”€ embedding_generator.py    # Sentence transformer wrapper
â”œâ”€â”€ evaluation/                   # Performance metrics
â”‚   â””â”€â”€ metrics.py                # RAG evaluation functions
â”œâ”€â”€ generator/                    # Answer generation
â”‚   â””â”€â”€ answer_generator.py       # LLM response generation
â”œâ”€â”€ ingestion/                    # Data ingestion
â”‚   â””â”€â”€ pdf_ingestion.py          # PDF processing pipeline
â”œâ”€â”€ retriever/                    # RAG retrieval logic
â”‚   â””â”€â”€ rag_retriever.py          # Main RAG implementation
â”œâ”€â”€ vector_store/                 # Database operations
â”‚   â””â”€â”€ chroma_db.py              # ChromaDB wrapper
â”œâ”€â”€ notebooks/                    # Jupyter notebooks
â”‚   â””â”€â”€ evaluation.ipynb          # Analysis and testing
â”œâ”€â”€ data/                         # Data directory (gitignored)
â”‚   â””â”€â”€ world_bank_pdfs/          # Source PDFs
â”œâ”€â”€ embeddings/                   # Embedding cache (gitignored)
â”œâ”€â”€ vector_store/                 # Database storage (gitignored)
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                    # Git ignore rules
â”œâ”€â”€ LICENSE                       # MIT License
â””â”€â”€ README.md                     # This file
```

## âš™ï¸ Configuration

### Environment Variables
Create a `.env` file (optional):
```env
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
EMBEDDING_MODEL=all-MiniLM-L6-v2
CHROMA_PERSIST_DIR=vector_store/chroma_db
```

### Customization Options
- **Embedding Models**: Change in `EmbeddingGenerator` initialization
- **Chunking Strategy**: Modify `text_chunker.py` parameters
- **LLM Settings**: Adjust temperature, max_tokens in `rag_retriever.py`
- **Retrieval Parameters**: Tune top_k, similarity thresholds

## ğŸ§ª Testing

### Available Test Scripts
```bash
# Test individual components
python test_ollama_connection.py      # Test LLM connection
python test_rag_simple.py             # Test basic RAG pipeline
python test_final_rag.py              # Test complete system
python test_streamlit_simple.py       # Test Streamlit interface
```

### Performance Benchmarks
- **Query Processing**: 2-5 seconds
- **Document Retrieval**: 1-3 seconds  
- **Answer Generation**: 5-30 seconds (depends on model)
- **Total Response Time**: 10-60 seconds

## ğŸ”§ Troubleshooting

### Common Issues

**Ollama Connection Failed**
```bash
# Check if Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama
ollama serve
```

**Import Errors**
```bash
# Reinstall dependencies
pip install -r requirements.txt --force-reinstall
```

**Vector Database Issues**
```bash
# Clear and rebuild database
rm -rf vector_store/chroma_db
python vector_store/chroma_db.py
```

**Memory Issues**
- Reduce `top_k` parameter in retrieval
- Use smaller embedding models
- Increase system RAM

### Performance Optimization
- **GPU Acceleration**: Install GPU versions of PyTorch
- **Caching**: Enable embedding caching for repeated queries
- **Batch Processing**: Process multiple queries simultaneously
- **Model Selection**: Use smaller/faster models for quick responses

## ğŸ“ˆ Features

### Core Capabilities
- âœ… **Semantic Search**: Advanced similarity matching
- âœ… **Context-Aware Answers**: LLM-powered response generation
- âœ… **Source Citation**: Precise document and page references
- âœ… **Performance Metrics**: Real-time system monitoring
- âœ… **Interactive Interface**: Modern web-based UI
- âœ… **Quality Assessment**: Automated answer evaluation

### Advanced Features
- ğŸ”„ **Multiple Models**: Support for various Ollama models
- ğŸ“Š **Analytics Dashboard**: Query performance and usage statistics
- ğŸ’¾ **Chat History**: Persistent conversation storage
- ğŸ›ï¸ **Configurable Parameters**: Customizable retrieval and generation settings
- ğŸ“± **Responsive Design**: Works on desktop and mobile devices

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. **Fork** the repository
2. **Create** a feature branch: `git checkout -b feature-name`
3. **Commit** changes: `git commit -am 'Add feature'`
4. **Push** to branch: `git push origin feature-name`
5. **Submit** a pull request

### Development Guidelines
- Follow PEP 8 style guidelines
- Add tests for new features
- Update documentation
- Ensure all tests pass

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **World Bank** for providing the Development Reports
- **Ollama** for local LLM serving capabilities
- **ChromaDB** for efficient vector storage
- **SentenceTransformers** for high-quality embeddings
- **Streamlit** for the excellent web framework

## ğŸ“ Support

For questions, issues, or suggestions:
- ğŸ› **Bug Reports**: Open an issue on GitHub
- ğŸ’¡ **Feature Requests**: Open an issue with the "enhancement" label
- ğŸ“§ **General Questions**: Use GitHub Discussions

---
